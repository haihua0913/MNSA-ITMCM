{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64aebc1-5be4-451f-bdcb-e6086751d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: bertopic in /environment/miniconda3/lib/python3.10/site-packages (0.16.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /environment/miniconda3/lib/python3.10/site-packages (from bertopic) (1.24.1)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in /environment/miniconda3/lib/python3.10/site-packages (from bertopic) (0.8.37)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /environment/miniconda3/lib/python3.10/site-packages (from bertopic) (0.5.6)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /environment/miniconda3/lib/python3.10/site-packages (from bertopic) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /environment/miniconda3/lib/python3.10/site-packages (from bertopic) (1.3.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /environment/miniconda3/lib/python3.10/site-packages (from bertopic) (4.66.4)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /environment/miniconda3/lib/python3.10/site-packages (from bertopic) (3.0.1)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /environment/miniconda3/lib/python3.10/site-packages (from bertopic) (5.22.0)\n",
      "Requirement already satisfied: cython<3,>=0.27 in /environment/miniconda3/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (0.29.37)\n",
      "Requirement already satisfied: scipy>=1.0 in /environment/miniconda3/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.0 in /environment/miniconda3/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /environment/miniconda3/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /environment/miniconda3/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /environment/miniconda3/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
      "Requirement already satisfied: packaging in /environment/miniconda3/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic) (24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /environment/miniconda3/lib/python3.10/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /environment/miniconda3/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.42.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /environment/miniconda3/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (2.0.1+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /environment/miniconda3/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.23.4)\n",
      "Requirement already satisfied: Pillow in /environment/miniconda3/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (9.3.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /environment/miniconda3/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /environment/miniconda3/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
      "Requirement already satisfied: filelock in /environment/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /environment/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /environment/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
      "Requirement already satisfied: requests in /environment/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /environment/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.8.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /environment/miniconda3/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in /environment/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: sympy in /environment/miniconda3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.11.1)\n",
      "Requirement already satisfied: networkx in /environment/miniconda3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0)\n",
      "Requirement already satisfied: jinja2 in /environment/miniconda3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /environment/miniconda3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.0.0)\n",
      "Requirement already satisfied: cmake in /environment/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.25.0)\n",
      "Requirement already satisfied: lit in /environment/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (15.0.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /environment/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /environment/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /environment/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /environment/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /environment/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /environment/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /environment/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5e360a-b732-42f4-a92e-530c5a030fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81ee140-c0d5-4bf3-b3e2-e3819a7552e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-09 18:04:17.371403: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-09 18:04:18.270512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "685be059-c871-4ef1-9137-7d37231fab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = 'papers_all_tags.csv' # 替换为您的CSV文件路径\n",
    "column_name = 'title' # 替换为您要读取的列名\n",
    "year_limit = 2021 # 替换为年份限制\n",
    "\n",
    "listdata = []\n",
    "\n",
    "with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        year = int(row['year'])\n",
    "        if year < year_limit:\n",
    "            listdata.append(row[column_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc2b8bc-481a-4a42-a196-aacefe559c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A uridine kinase-deficient mutant of 3T3 and a selective method for cells containing the enzyme', 'spoT, a new genetic locus involved in the stringent response in E. coli']\n"
     ]
    }
   ],
   "source": [
    "print(listdata[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a002d77-37ec-452c-aa31-2ed9b1709c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 476/476 [00:09<00:00, 48.94it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   sentence_model = sentence_model.to(torch.device(\"cuda\"))\n",
    "\n",
    "embeddings = sentence_model.encode(listdata, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c45d413-e335-4131-b5de-0a32c03f3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0, metric='cosine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29f27d52-5c5b-453d-a960-bc765d06eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1b3d3b5-d293-469e-982e-cdd0ba2e1767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=1, ngram_range=(2, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ff6da65-4a66-4bac-a031-23b1ee6dffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f319ff2f-1704-4a20-84ca-5f230c7e791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "# KeyBERT\n",
    "keybert_model = KeyBERTInspired()\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c074ddd0-3f84-4a8d-830a-483f455a2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "    embedding_model=sentence_model,    # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,              # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,        # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,  # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,          # Step 5 - Extract topic words\n",
    "    calculate_probabilities=True,\n",
    "    representation_model=representation_model,\n",
    "    n_gram_range=(1, 3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101c781a-f0a0-48cf-aaed-34a21f327617",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probabilities = topic_model.fit_transform(listdata, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56f13cef-f8f8-48c5-8463-5fe5998f33a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 18:05:19,607 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "topic_model.save(\"r5_cell_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31a649-9c7c-4291-affa-db1d2bd35224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "952cf296-b61c-4a98-a326-554b2f28f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#减少异常值（-1）\n",
    "# Reduce outliers with pre-calculate embeddings instead\n",
    "new_topics = topic_model.reduce_outliers(listdata, topics, strategy=\"embeddings\", embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43f998ee-11e2-4b56-b618-64daf90d1099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 18:05:33,245 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    }
   ],
   "source": [
    "topic_model.update_topics(listdata, topics=new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "742c3e76-599b-4333-9881-2010774753e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 18:05:35,028 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "topic_model.save(\"r5_cell_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54983da7-768e-4649-afab-fd350a53eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = BERTopic.load(\"r5_cell_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a25be75e-75a2-4103-befc-015dfd47d1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>94</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>92</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>95</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>96</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>99</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count\n",
       "16       0    761\n",
       "6        1    571\n",
       "17       2    547\n",
       "24       5    455\n",
       "47       3    415\n",
       "..     ...    ...\n",
       "39      94     45\n",
       "72      92     43\n",
       "103     95     43\n",
       "86      96     41\n",
       "102     99     41\n",
       "\n",
       "[104 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.get_topic_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a61a771-28a8-478a-842d-96402ad6b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05fa8a66-2872-407b-a069-c989bab98659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_model.visualize_topics().write_html('modelviz.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb75fc-535a-454d-bd84-467de3c432dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8f13e74-6cbc-43bb-a7ab-0269e3384119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# 加载模型\n",
    "topic_model = BERTopic.load(\"r5_cell_title\")\n",
    "\n",
    "# 读取数据\n",
    "csv_file = 'papers_all_tags.csv'  # 替换为您的CSV文件路径\n",
    "column_name = 'title'  # 替换为您要读取的列名\n",
    "year_limit = 2020  # 替换为年份限制\n",
    "\n",
    "newdata = []\n",
    "\n",
    "with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        year = int(row['year'])\n",
    "        if year > year_limit:\n",
    "            newdata.append(row[column_name])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c92d604-d251-4a8a-b8b6-0b8b3b6045c4",
   "metadata": {},
   "source": [
    "获取文档-主题矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ee0d602-f280-4fb0-8da1-68a1590a5bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to similar_topics.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# 求出主题相似度并保存到 CSV 文件\n",
    "with open('r5_similar_topics.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Query', 'Similar Topic', 'Similarity'])\n",
    "    for query in newdata:\n",
    "        similar_topics, similarity = topic_model.find_topics(query, top_n=104)\n",
    "        for topic, sim in zip(similar_topics, similarity):\n",
    "            writer.writerow([query, topic, sim])\n",
    "\n",
    "print(\"Results saved to similar_topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc8b0908-5eea-442b-b411-591754121653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_topic_similarities(topic_model):\n",
    "    \"\"\"计算所有主题之间的相似度\n",
    "\n",
    "    参数：\n",
    "        topic_model: 已拟合的 BERTopic 实例。\n",
    "\n",
    "    返回：\n",
    "        similarity_matrix: 相似度矩阵，其中元素 [i, j] 表示主题 i 和主题 j 之间的相似度。\n",
    "\n",
    "    示例：\n",
    "    ```python\n",
    "    topic_similarities = get_topic_similarities(topic_model)\n",
    "    print(topic_similarities)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    if topic_model.topic_embeddings_ is not None:\n",
    "        embeddings = np.array(topic_model.topic_embeddings_)[topic_model._outliers:]\n",
    "    else:\n",
    "        embeddings = topic_model.c_tf_idf_[topic_model._outliers:]\n",
    "\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdc45782-7da7-4d6c-bbc8-7215165b9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def save_topic_similarities_to_csv(topic_model, filepath):\n",
    "    \"\"\"将所有主题之间的相似度保存到 CSV 文件\n",
    "\n",
    "    参数：\n",
    "        topic_model: 已拟合的 BERTopic 实例。\n",
    "        filepath: 保存 CSV 文件的路径。\n",
    "\n",
    "    示例：\n",
    "    ```python\n",
    "    save_topic_similarities_to_csv(topic_model, 'topic_similarities.csv')\n",
    "    ```\n",
    "    \"\"\"\n",
    "    if topic_model.topic_embeddings_ is not None:\n",
    "        embeddings = np.array(topic_model.topic_embeddings_)[topic_model._outliers:]\n",
    "    else:\n",
    "        embeddings = topic_model.c_tf_idf_[topic_model._outliers:]\n",
    "\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    with open(filepath, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Topic'] + list(range(len(similarity_matrix))))\n",
    "        for i, row in enumerate(similarity_matrix):\n",
    "            writer.writerow([i] + list(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3faf9ee8-4214-4203-868c-791a036623d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_topic_similarities_to_csv(topic_model, filepath='r5_topic_sim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99542a9d-edba-44b0-a503-1a67eea250f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def select_topics(doc_topic_similarity, topic_similarity, n):\n",
    "    selected_topics = []\n",
    "    remaining_topics = list(topic_similarity.keys())\n",
    "\n",
    "    max_similarity = float('-inf')\n",
    "    selected_topic = None\n",
    "\n",
    "    # 选择与文档相关性最大的主题作为第一个主题\n",
    "    for topic in remaining_topics:\n",
    "        if topic in doc_topic_similarity:\n",
    "            similarity = doc_topic_similarity[topic]\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                selected_topic = topic\n",
    "\n",
    "    if selected_topic is not None:\n",
    "        selected_topics.append(selected_topic)\n",
    "        remaining_topics.remove(selected_topic)\n",
    "\n",
    "    # 选择余下的主题\n",
    "    while len(selected_topics) < n and remaining_topics:\n",
    "        max_difference = float('-inf')\n",
    "        selected_topic = None\n",
    "\n",
    "        for topic in remaining_topics:\n",
    "            avg_topic_similarity = np.mean([topic_similarity[topic][selected_topic] for selected_topic in selected_topics if selected_topic in topic_similarity[topic]])\n",
    "            difference = doc_topic_similarity[topic] - avg_topic_similarity\n",
    "            if difference > max_difference:\n",
    "                max_difference = difference\n",
    "                selected_topic = topic\n",
    "\n",
    "        if selected_topic is None:\n",
    "            break\n",
    "\n",
    "        selected_topics.append(selected_topic)\n",
    "        remaining_topics.remove(selected_topic)\n",
    "\n",
    "    return selected_topics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0fba363-97f1-4cef-b74d-801c627d04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "doc_topic_similarity = {}\n",
    "\n",
    "with open('r5_similar_topics.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        document = row['Query']\n",
    "        topic = row['Similar Topic']\n",
    "        similarity = float(row['Similarity'])\n",
    "\n",
    "        if document not in doc_topic_similarity:\n",
    "            doc_topic_similarity[document] = {}\n",
    "\n",
    "        doc_topic_similarity[document][topic] = similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "690b2ff1-e6ed-4c86-82f2-3531caa5149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "topic_similarity = {}\n",
    "\n",
    "with open('r5_topic_sim.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        topic1 = row['Topic']\n",
    "        topic_similarities = {}\n",
    "\n",
    "        for key, value in row.items():\n",
    "            if key != 'Topic':\n",
    "                topic2 = key\n",
    "                similarity = float(value)\n",
    "                topic_similarities[topic2] = similarity\n",
    "\n",
    "        topic_similarity[topic1] = topic_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a944b80-02bc-4c06-bf47-ce5416609636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果已保存到selected_topics.csv文件中。\n"
     ]
    }
   ],
   "source": [
    "# 样本数据\n",
    "doc_topic_similarity = doc_topic_similarity\n",
    "topic_similarity = topic_similarity\n",
    "n = 5  # 要选择的主题数量\n",
    "\n",
    "\n",
    "# 为每个文档选择主题\n",
    "selected_topics = {}\n",
    "for document, topic_similarity_scores in doc_topic_similarity.items():\n",
    "    selected_topics[document] = select_topics(topic_similarity_scores, topic_similarity, n)\n",
    "\n",
    "# 保存结果到CSV文件\n",
    "with open('r5_selected_topics.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['文档', '主题集合', '主题集合与文档的相关性'])\n",
    "    for document, topics in selected_topics.items():\n",
    "        similarity_values = [doc_topic_similarity[document][topic] for topic in topics]\n",
    "        writer.writerow([document, topics, similarity_values])\n",
    "\n",
    "print(\"结果已保存到selected_topics.csv文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ce8d4-7829-419c-8354-0714a1425e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics().write_html('review_cell_modelviz.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ce857-b2bb-4690-b53c-015178b67f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
